{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanpajedrez/pytorch_learning/blob/main/08_pytorch_paper_replicating_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbQbDLcBxvbs"
      },
      "source": [
        "# 08. Milestone Paper Replication: PyTorch Replicating Paper\n",
        "\n",
        "The goal of machine learning research paper replicating is: turn an ML research paper into usable code.\n",
        "\n",
        "In this notebook, we're going to be replicating the Vision Transformer (ViT) architecture with PyTorch:\n",
        "\n",
        "Paper link: https://arxiv.org/pdf/2010.11929\n",
        "Paper Abs: https://arxiv.org/abs/2010.11929\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYfJlexhyobt"
      },
      "source": [
        "## 0. Get Setup\n",
        "Let's import code we've previously written + required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_Cz3gtyyHgz",
        "outputId": "7494ef94-f646-45a7-b891-8a88a5d3a6cc"
      },
      "outputs": [],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "  import torch\n",
        "  import torchvision\n",
        "  assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
        "  assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "  print(f\"torch version: {torch.__version__}\")\n",
        "  print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "  print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "  !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "  import torch\n",
        "  import torchvision\n",
        "  print(f\"torch version: {torch.__version__}\")\n",
        "  print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZJjUJ0Py1Ft",
        "outputId": "f77b2a4d-4f25-44c5-81f2-b48abf7f1e1e"
      },
      "outputs": [],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Y6Bdbc_AzZUs",
        "outputId": "6d6709bd-3afc-48fa-bdec-97c0efdb6c13"
      },
      "outputs": [],
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnDs4_haRwSu",
        "outputId": "c8443107-f2ee-4f24-aed1-40ecec3e6744"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FP-903yziNA"
      },
      "source": [
        "## 1. Get data\n",
        "\n",
        "The whole goal of what we're trying to do is to replicate the ViT transformer architecture for our FoodVision Mini problem.\n",
        "\n",
        "To do that, we need some data.\n",
        "\n",
        "Namely, the pizza, steak and sushi images we've been using so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asYqC0TDz3qI",
        "outputId": "87249efc-6035-4e7c-be3e-ef91e4b38db2"
      },
      "outputs": [],
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_HpnZWq04qS"
      },
      "outputs": [],
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCClIfSb1F_b",
        "outputId": "2c14d037-65e8-477d-c4f4-8dfacf78d861"
      },
      "outputs": [],
      "source": [
        "train_dir, test_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR2Uy70b1HFO"
      },
      "source": [
        "## 2. Create Datasets and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMLDIReV1Zjb",
        "outputId": "de3f522c-b15e-4914-a1f4-662c3715689c"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from going_modular.going_modular import data_setup\n",
        "\n",
        "# Create image size\n",
        "IMG_SIZE = 224 # come from Table 3 of the ViT paper.\n",
        "\n",
        "# Create transforms pipeline\n",
        "manual_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "print(f\"Manually created transforms: {manual_transform}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHokdwrf3KBU",
        "outputId": "12e2d23b-bf06-47c2-d4a9-5b0a8facae61"
      },
      "outputs": [],
      "source": [
        "# Create a batch size of 32 (the paper uses 4096 but this may be too big for our smaller hardware... we scale regarding our hardware).\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "  train_dir = train_dir,\n",
        "  test_dir = test_dir,\n",
        "  transform = manual_transform,\n",
        "  batch_size = BATCH_SIZE\n",
        ")\n",
        "\n",
        "len(train_dataloader), len(test_dataloader), class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_sVIvjA3ohV"
      },
      "source": [
        "### 2.3 Visualize a single image\n",
        "\n",
        "As always, let's adhere to the motto *visualize, visualize, visualize!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0trZl7mg34ar",
        "outputId": "d719ede4-fce8-4fa2-98fa-7f6868adf99e"
      },
      "outputs": [],
      "source": [
        "# Get a batch of images\n",
        "image_batch, label_batch = next(iter(train_dataloader))\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "# View the single image and label shapes\n",
        "image.shape, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "eKeC3P9h8j5i",
        "outputId": "bce23013-0f5b-4c4d-9727-cda3437d0566"
      },
      "outputs": [],
      "source": [
        "# Plot the image with matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(image.permute(1, 2, 0)) # (color_channels, height, width) -> (height, width, color_channels)\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJC-eM-98-_Y"
      },
      "source": [
        "## 3. Replicating ViT: Overview\n",
        "\n",
        "Looking at a whole machine learning research paper can be intimidating.\n",
        "\n",
        "So in order to make it more undeerstandable, we can break it down into smaller pieces.\n",
        "\n",
        "* **Inputs:** - What goes into the model? (in our case, image tensors)\n",
        "* **Outputs:** - What comes out of the model/layer/block> (in our case, we want the model to output image classification labels).\n",
        "* **Layers** - Takes an input, manipulates it with a function (for example could be self-attention).\n",
        "* **Blocks** - A collection of layers:.\n",
        "* **Model** -  A collection of blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ3Ch2xHQodR"
      },
      "source": [
        "### 3.1 Overview: Pieces of the puzzle.\n",
        "\n",
        "* Figure 1: Visual Overview of the architecture\n",
        "* Four Equations: Math equations which define the function of layer/block.\n",
        "* Table 1/3: Different hyperparameters for the architecture/training.\n",
        "* Text.\n",
        "\n",
        "\n",
        "### Figure 1\n",
        "\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/08-vit-paper-figure-1-architecture-overview.png?raw=true)\n",
        "\n",
        "* Embedding = learnable representation (start with random numbers and improve them over time)\n",
        "\n",
        "### Four equations\n",
        "\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/08-vit-paper-four-equations.png?raw=true)\n",
        "\n",
        "**Equation 1:**\n",
        "An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_p \\in \\mathbb{R}^{N \\times\\left(P^2 \\cdot C\\right)}$, where ( $H, W$ ) is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.\n",
        "\n",
        "**Equation 1:**\n",
        "Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.\n",
        "\n",
        "```python\n",
        "# Equation 1\n",
        "x_input = [class_token, image_patch_1, image_patch_2, ..., image_patch_N] + [class_token_pos, image_patch_1_pos, image_patch_2_pos, ..., image_patch_N_pos]\n",
        "```\n",
        "\n",
        "**Equations 2&3:**\n",
        "The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski \\& Auli, 2019).\n",
        "\n",
        "In pseudocode:\n",
        "\n",
        "```python\n",
        "# Equation 2\n",
        "x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n",
        "\n",
        "# Equation3\n",
        "x_output_MLP_block = MLP_layer(LN_Layer(x_output_MSA_block)) + x_output_MSA_block\n",
        "```\n",
        "\n",
        "**Equation 4:**\n",
        "Similar to BERT's [class] token, we prepend a learnable embedding to the sequence of embedded patches ( $\\mathbf{z}_0^0=\\mathbf{x}_{\\text {class }}$ ), whose state at the output of the Transformer encoder ( $\\mathbf{z}_L^0$ ) serves as the image representation $\\mathbf{y}$ (Eq. 4). Both during pre-training and fine-tuning, a classification head is attached to $\\mathbf{z}_L^0$. The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.\n",
        "\n",
        "* MLP = multilayer perceptron = feed forward neural network with N number of layers.\n",
        "* MLP = one hidden layer at a time.\n",
        "* MLP = single line layer at fine-tuning time.\n",
        "\n",
        "In pseudocode:\n",
        "```python\n",
        "# Equation 4\n",
        "y = MLP(LN_Layer(x_output_MLP_block))\n",
        "```\n",
        "\n",
        "### Table 1\n",
        "\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/08-vit-paper-table-1.png?raw=true)\n",
        "\n",
        "* ViT-Base, ViT-Large, and ViT Huge are all different sizes of the same model architecture.\n",
        "* Layers - the number of transformer encoder layers\n",
        "* Hidden size $D$ - the embedding size throughout the architecture.\n",
        "* MLP size - the number of hidden units/neurons in the MLP.\n",
        "* Head - the number of multi-head self-attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjVsvR-CQr5D"
      },
      "source": [
        "## 4. Equation 1: Split data into patches and creating the class, position and patch embedding\n",
        "\n",
        "Layers = input -> function -> output\n",
        "\n",
        "What's the input shape?\n",
        "\n",
        "What's the output shape?`\n",
        "\n",
        "* Input shape: (224, 224, 3) -> Single image -> (height, width, colour_channels)\n",
        "* Output Shape: ??\n",
        "\n",
        "### 4.1 Calculate input and output shapes by hand\n",
        "\n",
        "**Equation 1:**\n",
        "An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_p \\in \\mathbb{R}^{N \\times\\left(P^2 \\cdot C\\right)}$, where ( $H, W$ ) is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1). We refer to the output of this projection as the patch embeddings.\n",
        "\n",
        "**Equation 1:**\n",
        "Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.\n",
        "\n",
        "* Input image: $H\\times{W}\\times{C}$ (height x width x color_channels)\n",
        "* Output Image: $\\mathbb{R}^{N \\times\\left(P^2 \\cdot C\\right)}$\n",
        "* H = height\n",
        "* W = width\n",
        "* C = color_channels\n",
        "* P = pathc size\n",
        "* N = number of patches = (heigth * width) / p^2\n",
        "* D = constant latent vector size = embedding dimension (see Table 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I29ZlcWcAWs",
        "outputId": "f9dd22f0-d18e-4bcf-d439-39ee12e9a6ce"
      },
      "outputs": [],
      "source": [
        "# Create example values\n",
        "height = 224\n",
        "width = 224\n",
        "colour_channels = 3\n",
        "patch_size = 16\n",
        "\n",
        "# Calculate the number of patches\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "number_of_patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfvk_axxh9s5",
        "outputId": "d512fac3-656e-4d39-a8de-eaa51c2b0f64"
      },
      "outputs": [],
      "source": [
        "# Input shape\n",
        "embedding_layer_input_shape = (height, width, colour_channels)\n",
        "\n",
        "# Output shape\n",
        "embedding_layer_output_shape = (number_of_patches, patch_size**2 * colour_channels)\n",
        "\n",
        "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
        "print(f\"Output shape (single 1D sequence of patches): {embedding_layer_output_shape} -> (number_patches, embedding_dimension)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQWNlXc3iQVU"
      },
      "source": [
        "### 4.2 Turning a single image into patches\n",
        "\n",
        "Let's visualize, visualize, visualize!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "MIweyNyxlday",
        "outputId": "607de7f1-1f09-4c73-9c2a-567f1333af07"
      },
      "outputs": [],
      "source": [
        "# View a single image\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi-QMN4wll1G",
        "outputId": "d165d3a0-093b-4c8a-c97e-c5011efb4d99"
      },
      "outputs": [],
      "source": [
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "jpqpvm_jloHr",
        "outputId": "6a730d5f-899e-4e53-b82b-3212e4e66880"
      },
      "outputs": [],
      "source": [
        "# Get the top row of the image\n",
        "image_permuted = image.permute(1, 2, 0) # convert images to color channels last  (C, H, W) -> (H, W, C)\n",
        "print(image_permuted.shape)\n",
        "\n",
        "# Index to plot the top row of pixels\n",
        "patch_size = 16\n",
        "plt.figure(figsize=(patch_size, patch_size))\n",
        "plt.imshow(image_permuted[:patch_size, :, :])\n",
        "plt.axis(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "yf_p4qujmg0j",
        "outputId": "5eb76624-0830-43c8-d0d3-216e506f7f19"
      },
      "outputs": [],
      "source": [
        "# Setup code to plot top row as patches\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size / patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\\n Patch_size: {patch_size}\\n pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(nrows = 1,\n",
        "                        ncols=img_size//patch_size, # one column for each patch\n",
        "                        sharex=True,\n",
        "                        sharey=True,\n",
        "                        figsize=(patch_size, patch_size)\n",
        "                        )\n",
        "\n",
        "# Iterate through number of patches in the top row\n",
        "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
        "  axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :])\n",
        "  axs[i].set_xlabel(i+1) # set the patch label\n",
        "  axs[i].set_xticks([])\n",
        "  axs[i].set_yticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cnvFA_Pfnz6q",
        "outputId": "e717be31-5d9e-42c9-ddcc-21444a173bd9"
      },
      "outputs": [],
      "source": [
        "# Setup code to plot whole image as patches\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "num_patches = img_size / patch_size\n",
        "assert img_size % patch_size == 0, \"Image size must be divisilbe by patch size\"\n",
        "print(f\"Number of patches per row: {num_patches}\")\n",
        "print(f\"Number of patches per column: {num_patches}\")\n",
        "print(f\"Number of total patches: {num_patches**2}\")\n",
        "print(f\"Patch_size: {patch_size} pixels x {patch_size} pixels\")\n",
        "\n",
        "# Create a series of subplots\n",
        "fig, axs = plt.subplots(nrows = int(img_size / patch_size),\n",
        "                        ncols = int(img_size / patch_size),\n",
        "                        figsize = (num_patches, num_patches),\n",
        "                        sharex = True,\n",
        "                        sharey = True)\n",
        "\n",
        "# Loop through height and width of the image\n",
        "for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n",
        "  for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n",
        "    # Plot the permuted image on the different axis\n",
        "    img_plot = image_permuted[patch_height:patch_height+patch_size, # Iterate through the height dim\n",
        "                                  patch_width: patch_width + patch_size, # Iterate through the width\n",
        "                                  :] # Get all the colour channels\n",
        "    axs[i, j].imshow(img_plot)\n",
        "\n",
        "    # Set up label information for each subplot (patch)\n",
        "    axs[i, j].set_ylabel(i + 1,\n",
        "                         rotation=\"horizontal\",\n",
        "                         horizontalalignment = \"right\",\n",
        "                         verticalalignment = \"center\")\n",
        "    axs[i, j].set_xlabel(j + 1)\n",
        "    axs[i, j].set_xticks([])\n",
        "    axs[i, j].set_yticks([])\n",
        "    axs[i, j].label_outer()\n",
        "\n",
        "# Setup a title for the plot\n",
        "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize = 14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-plvUSpnM-7"
      },
      "source": [
        "### 4.3 Creating images patches and turning them into patch embeddings\n",
        "\n",
        "Perhaps we could create the image patches and image patch embeddings in a single step using `torch.nn.Conv2d()` and setting the kernel size and stride parameters to `patch_size`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guYvbfZSkfnm",
        "outputId": "12a2d79b-510c-4e8e-f529-8958d79b6004"
      },
      "outputs": [],
      "source": [
        "# Create conv2d layer to turn image into patches of learnable feature maps (embeddings)\n",
        "from torch import nn\n",
        "\n",
        "# Set the patch size\n",
        "patch_size = 16\n",
        "\n",
        "# Create a conv2d layer with hyperparameters from the ViT paper\n",
        "conv2d = nn.Conv2d(in_channels=3, # for color images\n",
        "                   out_channels = 768, # D size from Table 1,\n",
        "                   kernel_size = patch_size,\n",
        "                   stride = patch_size,\n",
        "                   padding = 0)\n",
        "conv2d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "tX8dXEuQ3AGi",
        "outputId": "0def873c-8f87-488d-e2d6-9331bad1efb2"
      },
      "outputs": [],
      "source": [
        "# Visualize single image\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cobHg0P33LQq",
        "outputId": "fdefa772-efc5-4ee4-bac9-7d5dfbefe68c"
      },
      "outputs": [],
      "source": [
        "# Pass the image through the convolutional layer\n",
        "print(image.unsqueeze(0).shape)\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension -> (batch_size, color_channels, height, width)\n",
        "print(image_out_of_conv.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOpVh5gk3a5t",
        "outputId": "4bb953c7-bac2-4376-c0d6-2cfae6fd59cf"
      },
      "outputs": [],
      "source": [
        "14*14"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMLRXxu74g1C"
      },
      "source": [
        "Now we've passed a signle image to our `conv2d` layer, it's shape is:\n",
        "\n",
        "```python\n",
        "torch.Size([1, 768, 14, 14]) # [batch_size, embedding_dim, feature_map_height, feature_map_width]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "WFscG94g3fHJ",
        "outputId": "126d581f-a207-49b6-b2b6-6e8462d7e2f0"
      },
      "outputs": [],
      "source": [
        "# Plot random convolutional feature maps\n",
        "import random\n",
        "random_indexes = random.sample(range(0, 768-10), k = 5)\n",
        "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
        "\n",
        "# Create plot\n",
        "fig, axs = plt.subplots(nrows = 1,\n",
        "                        ncols = 5,\n",
        "                        figsize = (12, 12))\n",
        "\n",
        "# Plot random image feature maps\n",
        "for i, idx in enumerate(random_indexes):\n",
        "  image_conv_feature_map = image_out_of_conv[:, idx, :, :] # Index on the output tensor of the conv2d layer\n",
        "  axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy()) # Remove batch dimension, and remove from grad tracking/switch to numpy for matplotlib\n",
        "  axs[i].set(xticklabels = [], yticklabels = [], xticks = [], yticks = [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBpzKJ1I5TpP",
        "outputId": "aa6b2fb1-20de-4dad-9579-ca5a487d19cf"
      },
      "outputs": [],
      "source": [
        "# Get a single feature map in tensor form\n",
        "single_feature_map = image_out_of_conv[:, 0, :, :]\n",
        "single_feature_map, single_feature_map.requires_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moVOPPHt1fTx"
      },
      "source": [
        "### 4.4 Flattening the patch embedding with `torch.nn.Flatten()`\n",
        "\n",
        "Right now we've gotten a series of convolutional feature maps (patch embeddings) that we want to flatten into a sequence of patch embeddings to satisfy the input criteria of the ViT Transformer Encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jLQWKh-17L5",
        "outputId": "7ee95941-bd15-47a9-ca6f-151f27cf0f60"
      },
      "outputs": [],
      "source": [
        "print(f\"{image_out_of_conv.shape} -> (batch_size, embedding_dim, feature_map_height, feature_map_width)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mP1oVW4Vt86"
      },
      "source": [
        "Want: (batch_size, number_of_patches, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV0qk81hV4rI",
        "outputId": "c57e523f-d893-48b4-e8c9-efb1965920e0"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "flatten_layer = nn.Flatten(start_dim = 2,\n",
        "                           end_dim = 3)\n",
        "flatten_layer(image_out_of_conv).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "ShOqcr-qWC01",
        "outputId": "d8ef181e-5a88-4271-8b96-c2b9eab00d27"
      },
      "outputs": [],
      "source": [
        "# Put everything together\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)\n",
        "print(f\"Original image shape: {image.shape}\")\n",
        "\n",
        "# Turn image into feature maps\n",
        "image_out_of_conv = conv2d(image.unsqueeze(0)) # Add a batch dimension\n",
        "print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n",
        "\n",
        "# Flatten the feature maps\n",
        "image_out_of_conv_flattened = flatten_layer(image_out_of_conv)\n",
        "print(f\"Flattened feature map shape: {image_out_of_conv_flattened.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNrlPBxeWist",
        "outputId": "34179e3f-24b5-487c-816f-bc3fdc300ef5"
      },
      "outputs": [],
      "source": [
        "# Rearrange of output flattened\n",
        "image_out_of_conv_flattened_permuted = image_out_of_conv_flattened.permute(0, 2, 1)\n",
        "print(f\"{image_out_of_conv_flattened_permuted.shape} -> (batch_size, number_of_patches, embedding_dim)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "urs1d9SAW98B",
        "outputId": "0b15436f-fc47-43e4-e84f-5c094479ab8e"
      },
      "outputs": [],
      "source": [
        "single_flattened_feature_map = image_out_of_conv_flattened_permuted[:, :, 0] # Select (batch_size, embedding_dim, single_idx_patch)\n",
        "plt.figure(figsize=(22, 22))\n",
        "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
        "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
        "plt.axis(False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM1FyAMnXel3"
      },
      "source": [
        "### 4.5 Turning the ViT patch embedding layer into a PyTorch module\n",
        "\n",
        "We want this module to do a few things:\n",
        "1. Create a class called `PatchEmbedding` that inherits from `nn.Module`.\n",
        "2. Initialize with appropiate hyperparameters, such as channels, embedding dimension, patch size.\n",
        "3. Create a layer to turn an image into embedded patches using `nn.Conv2d()`\n",
        "4. Create a layer to flatten the feature maps of the output of the layer in 3.\n",
        "5. Define a `forward()` that defines the forward computatoin (e.g pass through layer from 3 to 4).\n",
        "6. Make sure the output shape of the layer reflects the required output shape of the patch embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIbQyMriZnge"
      },
      "outputs": [],
      "source": [
        "# 1. Create a class called PatchEmbedding\n",
        "class PatchEmbedding(nn.Module):\n",
        "  # 2. Initialize the layer with appropiate hyperparameters\n",
        "  def __init__(self,\n",
        "               in_channels:int = 3,\n",
        "               patch_size:int = 16,\n",
        "               embedding_dim: int = 768): # from Table 1 for ViT-Base\n",
        "    \"\"\"\n",
        "    Turns a 2D input image into a 1D sequence learnable embeddinf vector.\n",
        "\n",
        "    Args:\n",
        "      in_channels (int): number of color channels for the input image. (3 for RGB)\n",
        "      patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
        "      embedding_dim (int): Size of the embedding dimension. Defaults to 768.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # Store all hyperparameters\n",
        "    self.in_channels = in_channels\n",
        "    self.patch_size = patch_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # 3. Create a layer to turn an image into embedded patches\n",
        "    self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                             out_channels = embedding_dim,\n",
        "                             kernel_size = patch_size,\n",
        "                             stride = patch_size,\n",
        "                             padding = 0)\n",
        "\n",
        "    # 4. Create a layer to flatten feature map outputs of Conv2d\n",
        "    self.flatten = nn.Flatten(start_dim=2,\n",
        "                              end_dim=3)\n",
        "\n",
        "  # 5. Define the forward method\n",
        "  def forward(self, x): # Input x is (batch_size, channel_in, height, width) -> (N, 3, 224, 224)\n",
        "    # Create assertion to check that inputs are the correct shape\n",
        "    image_resolution = x.shape[-1]\n",
        "    assert image_resolution % self.patch_size == 0, f\"Input image size must be divisible by the patch size, image_shape: {image_resolution}, patch_size: {self.patch_size}\"\n",
        "\n",
        "    # Perform the forward pass\n",
        "    #print(x.shape)\n",
        "    x_patched = self.patcher(x)\n",
        "    #print(x_patched.shape)\n",
        "    x_flattened = self.flatten(x_patched)\n",
        "    #print(x_flattened.shape)\n",
        "\n",
        "    # 6. Make sure the returned sequence embedding dimensions are in the right order (batch_size, num_of_pacthes, embedding_dim)\n",
        "    return x_flattened.permute(0, 2, 1) # [batch_size, P^2*C, N] -> [batch_size, N, P^2*C]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0RZdPlhfKEj"
      },
      "outputs": [],
      "source": [
        "set_seeds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu51faLefLcN",
        "outputId": "75964b84-abea-443f-e445-b2a3c7b31cb7"
      },
      "outputs": [],
      "source": [
        "# Create amn instance of patch embedding layer\n",
        "patchify = PatchEmbedding(in_channels=3,\n",
        "                          patch_size = 16,\n",
        "                          embedding_dim=768)\n",
        "\n",
        "# Pass a single image through embedding layer\n",
        "print(f\"Input image size: {image.unsqueeze(0).shape}\")\n",
        "patch_embedded_image = patchify(image.unsqueeze(0)) # Add an extra batch dimension\n",
        "print(f\"Output patch embedded image shape: {patch_embedded_image.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaQ8eVavfkoW"
      },
      "outputs": [],
      "source": [
        "rand_img_tensor = torch.randn(1, 3, 224, 224)\n",
        "rand_img_tensor_bad = torch.randn(1, 3, 250, 250)\n",
        "\n",
        "# Let's try them\n",
        "#patchify(rand_img_tensor)\n",
        "#patchify(rand_img_tensor_bad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEQ-N-aUhKhB"
      },
      "source": [
        "### 4.6 Creating a class token embedding\n",
        "\n",
        "Want to: preprend a learnable class token to the start of the patch embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KWKTm2_rfMK",
        "outputId": "0e56f36d-eedf-407a-8c70-b3e67da9b2da"
      },
      "outputs": [],
      "source": [
        "patch_embedded_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQju-vUIrgKs",
        "outputId": "8af61658-6bbb-43d1-a341-3f23d4e7cd0f"
      },
      "outputs": [],
      "source": [
        "# Get the batch size and embedding dimension\n",
        "batch_size = patch_embedded_image.shape[0]\n",
        "embedding_dim = patch_embedded_image.shape[-1]\n",
        "batch_size, embedding_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FROL1Xf0sGVl",
        "outputId": "5a72e38d-3571-4730-b53b-9e26bfd8e84a"
      },
      "outputs": [],
      "source": [
        "# Create a class token embedding as a learnable parameter that shares the same size as the embedding Dimension (D)\n",
        "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dim),\n",
        "                           requires_grad=True)\n",
        "class_token.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3xEpLLOscMi",
        "outputId": "66ded84e-a068-47ec-b50a-d81e89221407"
      },
      "outputs": [],
      "source": [
        "patch_embedded_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a47nnDRuskQn",
        "outputId": "0757c96d-22df-4994-b65d-cdbe6ed206ed"
      },
      "outputs": [],
      "source": [
        "# Add the class token embedding to the front of the patch embedding\n",
        "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n",
        "                                                       dim = 1) # Number of patches dimension\n",
        "print(patch_embedded_image_with_class_embedding)\n",
        "print(f\"Class token embedding shape: {class_token.shape}\")\n",
        "print(f\"Patch embedding image with class embedding shape {patch_embedded_image_with_class_embedding.shape} -> (batch_size, class_token + number_of_patches, embedding_dim)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Z6LuGns_l6"
      },
      "source": [
        "### 4.7 Creating the position embedding\n",
        "\n",
        "Want to: create a series of 1D learnable position embeddings and to add them to the sequence of patch embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q29APbIupPlT",
        "outputId": "ef15168f-c35b-4914-b4ef-c1c44b3fc120"
      },
      "outputs": [],
      "source": [
        "# View the sequence of patch embeddings with the prepended class embedding\n",
        "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdPQkqsKpYiQ",
        "outputId": "b64185c9-8f10-4ed9-8531-ac4cc13a4512"
      },
      "outputs": [],
      "source": [
        "# Calculate the N (number_of_patches)\n",
        "number_of_patches = int((height * width) / patch_size**2)\n",
        "number_of_patches\n",
        "\n",
        "# Get the embedding dimension\n",
        "embedding_dim = patch_embedded_image_with_class_embedding.shape[-1]\n",
        "embedding_dim\n",
        "\n",
        "# Create the learnable 1D position embedding\n",
        "position_embedding = nn.Parameter(torch.ones(\n",
        "    batch_size,\n",
        "    number_of_patches + 1,\n",
        "    embedding_dim),\n",
        "    requires_grad=True)\n",
        "position_embedding, position_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEkuPIUrqdeH",
        "outputId": "0f827890-519b-445b-b4cf-c9bd1a101858"
      },
      "outputs": [],
      "source": [
        "# Add the position embedding to the patch and class token embedding\n",
        "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
        "patch_and_position_embedding, patch_and_position_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rb8zzrxqvqs"
      },
      "source": [
        "### 4.8 Putting it all together: from image to embedding\n",
        "\n",
        "We've written code to turn an image in a flattened sequence of patch embeddings.\n",
        "\n",
        "Now let's see it all in one cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-w-dkdlr4rw",
        "outputId": "f5998566-050d-464c-89ab-84c1918b53df"
      },
      "outputs": [],
      "source": [
        "# Set the seeds\n",
        "set_seeds()\n",
        "\n",
        "# Let's see our image\n",
        "print(f\"Original singular image: {image.shape} -> [colour_channels, height, width]\") # C x H x W\n",
        "height, width = image.shape[1], image.shape[2]\n",
        "\n",
        "# Let's add a batch dimension\n",
        "image_passed = image.unsqueeze(0)\n",
        "print(f\"Batch added singular image: {image_passed.shape} [batch_size, colour_channels, height, width]\")\n",
        "\n",
        "# Let's create our patch_embedding layer using our custom PatchEmbedding nn.module class\n",
        "# in_channels = 3\n",
        "# patch_size = 16\n",
        "# embedding_dim = 768\n",
        "patch_size = 16\n",
        "patchify = PatchEmbedding(in_channels=3,\n",
        "                          patch_size = patch_size,\n",
        "                          embedding_dim=768)\n",
        "patchified_image = patchify(image_passed)\n",
        "print(f\"Patchified image shape: {patchified_image.shape} -> [batch_size, num_patches, embedding_dim]\")\n",
        "\n",
        "# Obtain all the dims from the data\n",
        "batch_size = patchified_image.shape[0]\n",
        "num_patches = int((height * width)/patch_size**2)\n",
        "embedding_dim = patchified_image.shape[-1]\n",
        "\n",
        "# Now let's add the class token across the num_patches dimension\n",
        "class_token = nn.Parameter(\n",
        "    torch.randn(batch_size, 1, embedding_dim),\n",
        "    requires_grad = True)\n",
        "\n",
        "# Prepend/Concatanate it with patchified_image to make it [B x N x D] to [B x (N + 1) x D]\n",
        "patchified_image_with_class_token = torch.cat((class_token, patchified_image),\n",
        "                                               dim =1)\n",
        "print(f\"Patchified image with class token: {patchified_image_with_class_token.shape} -> [batch_size, num_patches + 1, embedding_dim]\")\n",
        "\n",
        "# Now let's create the position embedding and add it, shape must be [B x (N + 1) x D]\n",
        "position_embedding = nn.Parameter(\n",
        "    torch.randn(batch_size, num_patches + 1, embedding_dim),\n",
        "    requires_grad=True)\n",
        "print(f\"Position embedding shape: {position_embedding.shape} -> [batch_size, num_patches + 1, embedding_dim]\")\n",
        "\n",
        "# Now let's add it up and voila!\n",
        "patchified_image_with_class_token_and_position_embedding = patchified_image_with_class_token + position_embedding\n",
        "print(patchified_image_with_class_token_and_position_embedding.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSkp7nVdsUUU"
      },
      "source": [
        "## Equation 2: Multihead Self-attention MSA block\n",
        "\n",
        "* Multihead self-attention: which part of a sequence should pay the most attention to itself?\n",
        "  * In our case, we have a series of embedded images, which patch significantly relates to another patch.\n",
        "  * We want our neural network (ViT) to learn this relationship/representation.\n",
        "* To replicate MSA in PyTorch we can use: https://docs.pytorch.org/docs/stable/generated/torch.nn.modules.activation.MultiheadAttention.html\n",
        "* LayerNorm = Layer normalization (LayerNorm) is a technique to normalize the distribution of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy.\n",
        "  * Normalization = make everything have the same mean and same standard deviation.\n",
        "  * In PyTorch = https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "normalizes values over $D$ dimension, in our case, the $D$ dimension is the embedding dimension.\n",
        "  * When we normalize along the embedding dimension, it's like making all of the staircase the same size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnenTbRA2hzQ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttentionBlock(nn.Module):\n",
        "  '''\n",
        "  Create a multi-head self attention block (\"MSA\" block for short)\n",
        "  '''\n",
        "\n",
        "  def __init__(self,\n",
        "               embedding_dim:int = 768, # Hidden size D (embedding dimension) from Table 1 for ViT-Base\n",
        "               num_heads:int = 12, # Heads from Table 1 for ViT-base\n",
        "               attn_dropout:int = 0): # Heads from Table 1\n",
        "    super().__init__()\n",
        "\n",
        "    # Create the norm layer (LN)\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    # Create multihead attention (MSA) layer\n",
        "    self.multihead_attn = nn.MultiheadAttention(\n",
        "        embed_dim=embedding_dim,\n",
        "        num_heads=num_heads,\n",
        "        dropout=attn_dropout,\n",
        "        batch_first = True) # is the batch first? (batch, seq, feature) -> (batch, num_of_patches, embedding_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer_norm(x)\n",
        "    attn_output, _ = self.multihead_attn(query = x, # query embeddings\n",
        "                                         key = x, # key embeddings\n",
        "                                         value = x, # value embeddings\n",
        "                                         need_weights = False)\n",
        "    return attn_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEMGs5H37sfn",
        "outputId": "20dd2694-f383-42f0-e6d1-4b5ac0bbdd99"
      },
      "outputs": [],
      "source": [
        "# Create an instance MSA block\n",
        "multihead_self_attention_block = MultiHeadSelfAttentionBlock(\n",
        "    embedding_dim = 768,\n",
        "    num_heads = 2,\n",
        "    attn_dropout = 0)\n",
        "\n",
        "# Pass the patch and position image embedding sequence through MSA block\n",
        "patched_image_through_msa_block = multihead_self_attention_block(patchified_image_with_class_token_and_position_embedding)\n",
        "print(f\"Patched image with class token and position embedding shape: {patchified_image_with_class_token_and_position_embedding.shape}\")\n",
        "print(f\"Patched image through MSA block: {patched_image_through_msa_block.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTDW6vcy9NAe",
        "outputId": "29126562-1350-47ef-b2ad-6891f70b1ee7"
      },
      "outputs": [],
      "source": [
        "patchified_image_with_class_token_and_position_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV9IJMuZ9jga",
        "outputId": "16d722a7-d677-4037-f6fd-37e594259516"
      },
      "outputs": [],
      "source": [
        "patched_image_through_msa_block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz2AOsb_gdnf"
      },
      "source": [
        "## 6. Equation 3: Multilayer Perception (MLP block)\n",
        "\n",
        "* **MLP** = The MLP contains two layers with a GELU non-linearity (section 3.1):\n",
        "  * MLP = a quite broad term for a block with a series of layer(s), layers can be multiple or even only one hidden layer.\n",
        "  * Layers can mean: fully-connected, sense, linear, feed-forward, all are often similar names for the same thing. In PyTorch, they're often called `torch.nn.Linear`. In TensorFlow is `tf.keras.layers.Dense()`.\n",
        "  * GELU in PyTorch: https://docs.pytorch.org/docs/stable/generated/torch.nn.GELU.html\n",
        "  * MLP number of hidden units = MLP Size in Table 1\n",
        "* **Dropout** = Dropout, when used, is applied after every dense layer except for the qkv-projections and directly after adding positional-to patch embeddings. Hybrid models are trained with the exact setup as their ViT counterparts.\n",
        "  * Value for Dropout available in Table 3:\n",
        "\n",
        "In pseudocode:\n",
        "\n",
        "```python\n",
        "# MLP\n",
        "x = linear -> non-linear -> dropout -> linear -> dropout\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRZd2k769l9n"
      },
      "outputs": [],
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MLPBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Creates a layer normalized multilayer percepttron block (\"MLP block\" for short)\n",
        "  \"\"\"\n",
        "  # 2 Initialize the class hyperparameters from Table 1 and Table 3\n",
        "  def __init__(self,\n",
        "               embedding_dim:int = 768, # Hidden Size D from Table 1 for ViT-Base\n",
        "               mlp_size:int = 3072, # MLP size from Table 1 for ViT-Base\n",
        "               dropout = 0.1): # Dropout from Table 3 for ViT-Base\n",
        "    super().__init__()\n",
        "\n",
        "    # Create the norm layer\n",
        "    self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "    # Create the MLP\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(in_features=embedding_dim,\n",
        "                  out_features = mlp_size),\n",
        "        nn.GELU(),\n",
        "        nn.Dropout(p = dropout),\n",
        "        nn.Linear(in_features=mlp_size,\n",
        "                  out_features = embedding_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.layer_norm(x)\n",
        "    x = self.mlp(x)\n",
        "    return x\n",
        "    # return self.mlp(self.layer_norm(x)) # same as above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5v8J1xSu5V_",
        "outputId": "4a47ba89-c8ea-4e7a-9e8d-f8ec61a69c5b"
      },
      "outputs": [],
      "source": [
        "# Create an instance MLPBlock\n",
        "mlp_block = MLPBlock(embedding_dim = 768,\n",
        "                     mlp_size = 3072,\n",
        "                     dropout = 0.1)\n",
        "\n",
        "# Pass output the MSABlock through MLPBlock\n",
        "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
        "print(f\"patched image through MLP block: {patched_image_through_mlp_block.shape}\")\n",
        "print(f\"patched image through MSA block: {patched_image_through_msa_block.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_ss0jlivTV5",
        "outputId": "ecb06260-54ec-44dc-cae7-a57c9602cbcf"
      },
      "outputs": [],
      "source": [
        "patched_image_through_mlp_block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wXZKAKrvbYO",
        "outputId": "ce7f195a-27c9-4f17-fd2f-6b770d85f5a3"
      },
      "outputs": [],
      "source": [
        "patched_image_through_msa_block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVdd6n6EvdPy"
      },
      "source": [
        "## 7. Creating the Transformer Encoder\n",
        "\n",
        "The transformer Encoder is a combination of alternating blocks of MSA (equation 2) and MLP (equation 3).\n",
        "\n",
        "And there are residual connections between each block.\n",
        "\n",
        "* Encoder =  turn a sequence into learnable representation.\n",
        "* Decoder = go from learn representation back to some sort of sequence.\n",
        "* Residual connections = add a layer(s) input to its subsequent output, this enables the cration of deeper networks\n",
        "(prevents weights from getting too small).\n",
        "\n",
        "In pseudocode:\n",
        "\n",
        "```python\n",
        "# Transformer Encoder\n",
        "x_input -> MSA block -> [MSA_block_output + x_input] -> MLP_block -> [MLP_block_output + MSA_block_output + x_input] -> ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmxoYcwSzRTk"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               embedding_dim: int = 768, # Hidden size D from table 1, 768 for ViT-Base\n",
        "               num_heads:int = 12, # from table 1\n",
        "               mlp_size:int = 3072, # from table 1\n",
        "               mlp_dropout:int = 0.1, # from table 3\n",
        "               attn_dropout:int = 0):\n",
        "    super().__init__()\n",
        "\n",
        "    # Create MSA block (equation 2)\n",
        "    self.msa_block = MultiHeadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                 num_heads=num_heads,\n",
        "                                                 attn_dropout=attn_dropout)\n",
        "\n",
        "    # Create MLP block (equation 3)\n",
        "    self.mlp_block = MLPBlock(embedding_dim=embedding_dim,\n",
        "                            mlp_size=mlp_size,\n",
        "                            dropout=mlp_dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.msa_block(x) + x # residual/skip connection for equation 2\n",
        "    x = self.mlp_block(x) + x # residual/skip connection for equation 3\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsLkzNRJ8LpX",
        "outputId": "273f33ca-b748-4f5c-dc8a-1317f2e98fd7"
      },
      "outputs": [],
      "source": [
        "# Create an instance of TransformerEncoderBlock()\n",
        "transformer_encoder_block = TransformerEncoderBlock()\n",
        "\n",
        "# Get a summary using torchinfo summary\n",
        "summary(model = transformer_encoder_block,\n",
        "        input_size=(1, 197, 768), # (batch_size, number_of_patches, embedding_dim)\n",
        "        col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width = 20,\n",
        "        row_settings = [\"var_names\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJcn9OWX8uvf"
      },
      "source": [
        "### 7.2 Create a custom Transformer Encoder block layer with in-built PyTorch layers\n",
        "\n",
        "So far we've created a transformer encoder by hand.\n",
        "\n",
        "But because of how good the Transformer architecture is, PyTorch has implemented ready to use Transformer Encoder layers:\n",
        "* https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "* https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html\n",
        "* https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html\n",
        "\n",
        "We can create a Transformer Encoder with pure PyTorch layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2cCZJ9omdla",
        "outputId": "c36e69b6-a95d-40e4-85ed-113ead90c020"
      },
      "outputs": [],
      "source": [
        "# Create the same as above with torch.nn.TransformerEncoderLayer()\n",
        "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Embedding size of table 1\n",
        "                                                             nhead=12,  # heads from table 1\n",
        "                                                             dim_feedforward=3072, # MLP size of table 1\n",
        "                                                             dropout=0.1,\n",
        "                                                             activation=\"gelu\",\n",
        "                                                             batch_first=True,\n",
        "                                                             norm_first=True)\n",
        "torch_transformer_encoder_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoBMx4Ofn6vi",
        "outputId": "539c52c6-7376-4916-90c1-9f99e8331441"
      },
      "outputs": [],
      "source": [
        "# Get a summary using torchinfo summary\n",
        "summary(model = torch_transformer_encoder_layer,\n",
        "        input_size=(1, 197, 768), # (batch_size, number_of_patches, embedding_dim)\n",
        "        col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width = 20,\n",
        "        row_settings = [\"var_names\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9OJDOcEoCLL"
      },
      "source": [
        "Why spend all this time recreating the transformer encoder when we could've just made it with a single torch layer? practice!\n",
        "\n",
        "Practice. Practice. Practice\n",
        "\n",
        "Now we know how things are implemented behind the scences, we can tweak them if necessary\n",
        "\n",
        "What are the benefits of using a pre-built PyTorch Layer?\n",
        "* Less prone to errors (goes through a bunch of testing).\n",
        "* Potential better performance, speed ups and optimizations (boosts!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0A70slwqFjy"
      },
      "source": [
        "## 8. Putting it all together to create ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDhfLI7VqJVn"
      },
      "outputs": [],
      "source": [
        "# Create a ViT class\n",
        "class ViT(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size:int = 224, # Table 3 from the ViT paper\n",
        "               in_channels:int = 3,\n",
        "               patch_size:int = 16,\n",
        "               num_transformer_layers:int = 12, # Table 1 for \"Layers\" for ViT-base\n",
        "               embedding_dim:int = 768, # Hidden size D from Table 1 for ViT-base\n",
        "               mlp_size:int = 3072, # Table 1\n",
        "               num_heads:int = 12, # Table 1\n",
        "               attn_dropout = 0,\n",
        "               mlp_dropout:int = 0.1,\n",
        "               embedding_dropout:int=0.1, # Dropout for patch and position embeddings\n",
        "               num_classes:int = 1000): # number of classes in our classification problem\n",
        "    super().__init__()\n",
        "\n",
        "    # Make an assertion that the image size is compatible with the patch size\n",
        "    assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image_size: {img_size}, patch_size: {patch_size}\"\n",
        "\n",
        "    # Calculate the number of patches (height, width/patch^2)\n",
        "    self.num_patches = (img_size * img_size) // patch_size ** 2\n",
        "\n",
        "    # Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
        "    self.class_embedding = nn.Parameter(data = torch.randn(1, 1, embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "\n",
        "    # Create learnable position embedding\n",
        "    self.position_embedding = nn.Parameter(data = torch.randn(1, self.num_patches + 1, embedding_dim))\n",
        "\n",
        "    # Create dropout value\n",
        "    self.embedding_dropout = nn.Dropout(p = embedding_dropout)\n",
        "\n",
        "    # Create patch embedding layer\n",
        "    self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                        patch_size=patch_size,\n",
        "                                        embedding_dim = embedding_dim)\n",
        "\n",
        "    # Create the Transformer Encoder Block\n",
        "    self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
        "                                                                       num_heads=num_heads,\n",
        "                                                                       mlp_size=mlp_size,\n",
        "                                                                       mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
        "\n",
        "    # Create classifier head\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "        nn.Linear(in_features=embedding_dim,\n",
        "                  out_features=num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Get the batch size\n",
        "    batch_size = x.shape[0]\n",
        "\n",
        "    # Create the class token embedding and expand it to match batch size (equation 1)\n",
        "    class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimensions\n",
        "\n",
        "    # Create the patch embedding (equation 1)\n",
        "    x = self.patch_embedding(x)\n",
        "\n",
        "    # Concat class token embedding and patch embedding (equation 1)\n",
        "    x = torch.cat((class_token, x), dim = 1) # (batch_size, number_of_patches+1, embedding_dim)\n",
        "\n",
        "    # Add position embedding to class token and patch embedding\n",
        "    x = self.position_embedding + x\n",
        "\n",
        "    # Apply dropout to patch embedding (\"directly after adding positional-to patch embeddings\")\n",
        "    x = self.embedding_dropout(x)\n",
        "\n",
        "    # Pass position and patch embedding to Transformer Encoder Layer (equation 2 & 3)\n",
        "    x = self.transformer_encoder(x)\n",
        "\n",
        "    # Put 0th index logit through the classifier (equation 4)\n",
        "    x = self.classifier(x[:, 0])\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEUFHXx30m5h",
        "outputId": "f23d5e32-ca12-4078-93ad-6a67d0ce1c63"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "embedding_dim = 768\n",
        "class_embedding = nn.Parameter(data = torch.randn(1, 1, embedding_dim),\n",
        "                                       requires_grad=True)\n",
        "class_embedding_expanded = class_embedding.expand(batch_size, -1, -1)\n",
        "print(class_embedding.shape)\n",
        "print(class_embedding_expanded.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5SPERWkDBDz",
        "outputId": "1e1d5a9b-c924-4bbf-bfa5-10802f1e9426"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(size = (1, 197, 768))\n",
        "x[:, 0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EOaPYSHFFcu",
        "outputId": "81468979-521c-4437-8934-556970c1157b"
      },
      "outputs": [],
      "source": [
        "vit = ViT()\n",
        "vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bNtjExLFaAl",
        "outputId": "c60be7e4-10b9-469c-a687-e20f9b97bd48"
      },
      "outputs": [],
      "source": [
        "set_seeds()\n",
        "\n",
        "# Create a random image tensor with same shape as a single image\n",
        "random_image_tensor = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Create an instance of our ViT architecture with the number of classes we're working with (pizza, steak, and sushi)\n",
        "vit = ViT(num_classes=len(class_names))\n",
        "\n",
        "# Pass the random image tensor through the ViT\n",
        "print(vit(random_image_tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fel18N1aIs4_"
      },
      "source": [
        "### 8.1 Getting a visual summary of our ViT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZozo6S1JVEF",
        "outputId": "2a72828e-69b3-49de-f9dc-84fcd358e2b8"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Get a summary using torchinfo summary\n",
        "summary(model = ViT(num_classes=len(class_names)),\n",
        "        input_size=(1, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width = 20,\n",
        "        row_settings = [\"var_names\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHAmeQU2KDk-"
      },
      "source": [
        "## 9. Setting up training ode for our Custom ViT\n",
        "\n",
        "We've replicated the ViT architecture, now let's see how it performs on our FoodVision Mini data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dULqHPRPYZT0"
      },
      "source": [
        "### 9.1 Creating an optimizer\n",
        "\n",
        "The paper staes it uses the Adam optimizer (section 4, Training and Fine Tuning)\n",
        "with $B_1$ value of 0.9 and $B_2$ of 0.999 (defaults), and weight decay is 0.1.\n",
        "\n",
        "Weight decay = Regularization technique -> Prevents overfitting, usually the L2 norm of the weights, to the loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGZM1e7dYbsy"
      },
      "source": [
        "### 9.2 Creating a loss function\n",
        "\n",
        "The ViT paper doesn't actually mention what loss function they used.\n",
        "\n",
        "So since it's a multi-classification paper, we'll use the `torch.nn.CrossEntropyLoss()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "f_tiMjSgaW6b",
        "outputId": "02274fe2-ad3b-4a05-c895-2cb2cb96d429"
      },
      "outputs": [],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "c5e0ab07a5ad4e498e51c2c5d6d555f6",
            "016f7dfaacda4c10b88c6162adde4b72",
            "ebf3bebfe85347fd8928a6e96f8fbf1c",
            "2606f5896c9447bfaa3186fe17eb837a",
            "43993a2213e847edafb07f82cbe71d46",
            "3318ae0dfc73423e87975af247d91175",
            "a860ed8e7c7840a1b11bdc8e8acb908c",
            "a130005f1316418793f4e2f2cc1e4319",
            "89ff18a4a01c4be9950f63e21f2a5527",
            "13a5e58c2d724b6aabe3f28625b283a5",
            "27f353d049a043fdb1f0b8a3cb81ed21"
          ]
        },
        "id": "fzQxlmUuaj-9",
        "outputId": "e9e20bba-5e74-4e99-e7c9-30ffcfb3097c"
      },
      "outputs": [],
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "set_seeds()\n",
        "\n",
        "# This breaks if using cpu as the device since model is too big in memory :)))\n",
        "optimizer = torch.optim.Adam(params=vit.parameters(),\n",
        "                             lr=1e-3,\n",
        "                             betas = (0.9, 0.999),\n",
        "                             weight_decay=0.1)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "results = engine.train(model = vit,\n",
        "                       train_dataloader = train_dataloader,\n",
        "                       test_dataloader = test_dataloader,\n",
        "                       epochs = 10,\n",
        "                       loss_fn = loss_fn,\n",
        "                       optimizer = optimizer,\n",
        "                       device = device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enABCxI2gJSW"
      },
      "source": [
        "### 9.4 What our training setup is missing\n",
        "\n",
        "How is our training setup different to the ViT paper?\n",
        "\n",
        "We've replicated model architecture correctly.\n",
        "\n",
        "But what was different between our trianing procedure (to get such poor results) and the ViT paper training procedures to get such great results?\n",
        "\n",
        "The main things our training implementation is missing:\n",
        "\n",
        "Prevent underfitting:\n",
        "* Data - our setup uses far less data (225 vs millions)\n",
        "\n",
        "Prevent overfitting:\n",
        "* learning rate warmup - start with a low learning rate and increase to a base LR.\n",
        "* Learning rate decay - as your model gets closer to convergence, start to lower the learning rate.\n",
        "* Gradient Clipping - prevent gradients from getting too big."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tekoaf08g-bh"
      },
      "source": [
        "### 9.5 Plotting loss curves for our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "iSvFZTGKhCSX",
        "outputId": "0d4b1245-b32e-413a-932b-f4305878b1c9"
      },
      "outputs": [],
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBcAwwmnL5sG"
      },
      "source": [
        "## 10. Using a pretrained ViT from `torchvision.models`\n",
        "\n",
        "Generally in deep learning, if you can use a pretrained model from a large dataset on your own problem. it's often a good place to start.\n",
        "\n",
        "If you can find a pretrained model and use transfer learning, give it a go, it often achieves great results with little data.\n",
        "\n",
        "### 10.1 Why use a pretrained model?\n",
        "\n",
        "* Sometimes data is limited.\n",
        "* Limited training resources.\n",
        "* Get better results faster (sometimes)..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb5O3EH1MMe-",
        "outputId": "ba380653-4f52-4efd-e1c2-3219933c6038"
      },
      "outputs": [],
      "source": [
        "# Cost of a TPUv3 for 30 days\n",
        "cost = 30*24*8\n",
        "print(f\"Cost of renting TPUv3 for 30 straight days in {cost} USD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krsoW18JNG07",
        "outputId": "ad07fa52-d205-48eb-8744-e7fa0e60f81b"
      },
      "outputs": [],
      "source": [
        "# The following torch v0.12+ and torchvision 0.13+\n",
        "import torch\n",
        "import torchvision\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "llTLA94jOBuV",
        "outputId": "bd367e39-f1bd-4e7b-f02e-d34f649fd629"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2ORhGTpTP4l"
      },
      "source": [
        "### 10.2 Prepare a pretrained ViT for use with FoodVision Mini (turn it into a feature extractor)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaVwRUkyOGZ2",
        "outputId": "fe5537c8-3a2d-496f-8677-ccafb81ea970"
      },
      "outputs": [],
      "source": [
        "# Get pretrained models for ViT\n",
        "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # \"DEFAULT = best availbale\"\n",
        "\n",
        "# Setup a ViT model instance with pretrained weights\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights = pretrained_vit_weights).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2eCCbglObbr"
      },
      "outputs": [],
      "source": [
        "# Freeze the base parameters\n",
        "for parameter in pretrained_vit.parameters():\n",
        "  parameter.requires_grad = False\n",
        "\n",
        "# Update the classifier head\n",
        "set_seeds()\n",
        "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diugY7weOwXE",
        "outputId": "82efb91e-06fa-4635-e093-38782fdd1d57"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Get a summary using torchinfo summary\n",
        "summary(model = pretrained_vit,\n",
        "        input_size=(1, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width = 20,\n",
        "        row_settings = [\"var_names\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3O0U1QNO4JP"
      },
      "source": [
        "### 10.3 Preparing data for the pretrained Vit model\n",
        "\n",
        "When using a pretrained model, you want to make sure your data is formatted in the same way that the model was trained on.\n",
        "\n",
        "* https://docs.pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnSrqbHgTr13",
        "outputId": "eb29f4a5-a9b9-4ce3-bb28-fc67e8c4565d"
      },
      "outputs": [],
      "source": [
        "# Get automatic transforms from pretrained ViT weights\n",
        "vit_transforms = pretrained_vit_weights.transforms()\n",
        "vit_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qScGNMFgTyCc"
      },
      "outputs": [],
      "source": [
        "# Setup dataloaders\n",
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                     test_dir = test_dir,\n",
        "                                                                                                     transform = vit_transforms,\n",
        "                                                                                                     batch_size = 32) # Could set a higher batch size because using a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MNhXAOKUSj_"
      },
      "source": [
        "### 10.4 Train feature extractor ViT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "82167077325e4c40aa5381b0e9cb751e",
            "77ab4cad9b5546adb1082e34b75cfd1a",
            "2cc9d000d3054de0a249b33172560c6c",
            "ab645177b5e84f41b5b43095dc359b42",
            "76aa14e29138489583c9f0c29ec2a9b9",
            "d7a09e6e986c4d1cb932166c5f455832",
            "eb9ab7f07c5c4bc7b92237221b3d6407",
            "bddc7c5315bd4a88a345fbcaaedf2c7b",
            "850352e1a05d48c5b35efc23d98c1cb8",
            "8393bc542fae4d1e8c86aa95b8e31148",
            "4dbae860312141acb3010ee9dfbfbdca"
          ]
        },
        "id": "juGogfMmVQQ9",
        "outputId": "27ce8af7-b134-4b7e-ace5-2e1d9602e269"
      },
      "outputs": [],
      "source": [
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Create optimizer and loss function\n",
        "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the classifier head of pretrained ViT\n",
        "set_seeds()\n",
        "pretrained_vit_results = engine.train(model = pretrained_vit,\n",
        "                                      train_dataloader = train_dataloader_pretrained,\n",
        "                                      test_dataloader = test_dataloader_pretrained,\n",
        "                                      loss_fn = loss_fn,\n",
        "                                      optimizer = optimizer,\n",
        "                                      epochs = 10,\n",
        "                                      device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_vEUHo4V_IG"
      },
      "source": [
        "### 10.5 Plot the loss curves of our pretrained ViT feature extractor transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "-1wOH2W6Vpbl",
        "outputId": "55af216a-cb80-4b7b-c1a4-881f5c87a317"
      },
      "outputs": [],
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "plot_loss_curves(pretrained_vit_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I49EfDEnV1_O"
      },
      "source": [
        "### 10.6 Save our best performing ViT model\n",
        "\n",
        "Now we've got a model that performs quite well, how about we save it to file and then check it's filesize\n",
        "\n",
        "We want to check the filesize because fi we wanted to deploy a model to say a website/mobile application, we may have\n",
        "limitations on the size of the model we can deploy\n",
        "\n",
        "E.g a smaller model may be required due to compute restrictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p6l3T4XYX4y",
        "outputId": "9c4dab5d-534d-4e15-f1f9-a1fc48b9f196"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "from going_modular.going_modular import utils\n",
        "\n",
        "model_name_str = \"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\"\n",
        "\n",
        "utils.save_model(model = pretrained_vit,\n",
        "                target_dir = \"models\",\n",
        "                model_name = model_name_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juUReMGdYhmU",
        "outputId": "f92d08c5-be04-4387-fc83-8a820873dbf3"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Get the model size in bytes then convert to megabytes\n",
        "pretrained_vit_model_size = Path(\"models/\" + model_name_str).stat().st_size // (1024 * 1024)\n",
        "print(f\"Pretrained ViT model size: {pretrained_vit_model_size} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_TQ6Q88a_jR"
      },
      "source": [
        "Our pretrained ViT gets some of the best results we've seen so far on our FoodVision Mini problem, however, the model size is 11x larger than our next best performing model.\n",
        "\n",
        "Perhaps the larger model size might cause issues when we go to deploy it (e.h: hard to deploy such a large file/might not make predictions as fast a smaller model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D6S5nzga4zp"
      },
      "source": [
        "### 11. Predicting on a custom image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "mH8D3T6wZzJ7",
        "outputId": "decc914b-7ac5-48ba-f4b8-d5b1f291db9d"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Import function to make predictions on images and plot them\n",
        "from going_modular.going_modular.predictions import pred_and_plot_image\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = image_path / \"04-pizza-dad.jpeg\"\n",
        "\n",
        "# Download the image if it doesn't already exist\n",
        "if not custom_image_path.is_file():\n",
        "    with open(custom_image_path, \"wb\") as f:\n",
        "        # When downloading from GitHub, need to use the \"raw\" file link\n",
        "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
        "        print(f\"Downloading {custom_image_path}...\")\n",
        "        f.write(request.content)\n",
        "else:\n",
        "    print(f\"{custom_image_path} already exists, skipping download.\")\n",
        "\n",
        "# Predict on custom image\n",
        "pred_and_plot_image(model=pretrained_vit,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKz-oBkRbaVn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOQ2y9AdwIhzcNJ2vKdsMfu",
      "gpuType": "A100",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "016f7dfaacda4c10b88c6162adde4b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3318ae0dfc73423e87975af247d91175",
            "placeholder": "",
            "style": "IPY_MODEL_a860ed8e7c7840a1b11bdc8e8acb908c",
            "value": "100%"
          }
        },
        "13a5e58c2d724b6aabe3f28625b283a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2606f5896c9447bfaa3186fe17eb837a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13a5e58c2d724b6aabe3f28625b283a5",
            "placeholder": "",
            "style": "IPY_MODEL_27f353d049a043fdb1f0b8a3cb81ed21",
            "value": "10/10[00:27&lt;00:00,2.77s/it]"
          }
        },
        "27f353d049a043fdb1f0b8a3cb81ed21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cc9d000d3054de0a249b33172560c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bddc7c5315bd4a88a345fbcaaedf2c7b",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_850352e1a05d48c5b35efc23d98c1cb8",
            "value": 10
          }
        },
        "3318ae0dfc73423e87975af247d91175": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43993a2213e847edafb07f82cbe71d46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dbae860312141acb3010ee9dfbfbdca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76aa14e29138489583c9f0c29ec2a9b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77ab4cad9b5546adb1082e34b75cfd1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7a09e6e986c4d1cb932166c5f455832",
            "placeholder": "",
            "style": "IPY_MODEL_eb9ab7f07c5c4bc7b92237221b3d6407",
            "value": "100%"
          }
        },
        "82167077325e4c40aa5381b0e9cb751e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77ab4cad9b5546adb1082e34b75cfd1a",
              "IPY_MODEL_2cc9d000d3054de0a249b33172560c6c",
              "IPY_MODEL_ab645177b5e84f41b5b43095dc359b42"
            ],
            "layout": "IPY_MODEL_76aa14e29138489583c9f0c29ec2a9b9"
          }
        },
        "8393bc542fae4d1e8c86aa95b8e31148": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "850352e1a05d48c5b35efc23d98c1cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89ff18a4a01c4be9950f63e21f2a5527": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a130005f1316418793f4e2f2cc1e4319": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a860ed8e7c7840a1b11bdc8e8acb908c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab645177b5e84f41b5b43095dc359b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8393bc542fae4d1e8c86aa95b8e31148",
            "placeholder": "",
            "style": "IPY_MODEL_4dbae860312141acb3010ee9dfbfbdca",
            "value": "10/10[00:22&lt;00:00,2.25s/it]"
          }
        },
        "bddc7c5315bd4a88a345fbcaaedf2c7b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5e0ab07a5ad4e498e51c2c5d6d555f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_016f7dfaacda4c10b88c6162adde4b72",
              "IPY_MODEL_ebf3bebfe85347fd8928a6e96f8fbf1c",
              "IPY_MODEL_2606f5896c9447bfaa3186fe17eb837a"
            ],
            "layout": "IPY_MODEL_43993a2213e847edafb07f82cbe71d46"
          }
        },
        "d7a09e6e986c4d1cb932166c5f455832": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb9ab7f07c5c4bc7b92237221b3d6407": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebf3bebfe85347fd8928a6e96f8fbf1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a130005f1316418793f4e2f2cc1e4319",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89ff18a4a01c4be9950f63e21f2a5527",
            "value": 10
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
