{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanpajedrez/pytorch_learning/blob/main/06_pytorch_transfer_learning_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30e4N_7LlAnk"
      },
      "source": [
        "#06. Pytorch Transfer Learning\n",
        "\n",
        "What is transfer learning?\n",
        "\n",
        "Transfer learning involves takes the parameters of what one model has learned on another dataset and applying them to our own problem.\n",
        "\n",
        "* Pretrained model = foundation models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiIYH3bQmb_v",
        "outputId": "4adc988f-4caa-42af-8dc5-effd57c553e4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(torch.__version__) # Want 1.12+\n",
        "print(torchvision.__version__) # Want 0.13+"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y54WsGeynjgq"
      },
      "source": [
        "### Not necessary\n",
        "\n",
        "We are in 2025-10-24. We have versions waaay higher, so we are ok :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu8xLqqpm1_U"
      },
      "source": [
        "Now we've got the versions of torch and torchvision, we would download our codes from previous sections from going_modular, so we don't have to write it all again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_mu43s-ovnl"
      },
      "outputs": [],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bjcmcvrQowIx",
        "outputId": "4067f8be-06c7-4850-98a7-4087e5bf8955"
      },
      "outputs": [],
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gR-9sZDPZl6Q",
        "outputId": "350c3805-e63b-4f17-a0ac-b25349003e7d"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUoAm8UoZp95"
      },
      "source": [
        "## 1. Get data\n",
        "\n",
        "We need our pizza, steak, sushi data to build a transfer learning model on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giQdiTCRZ7Po",
        "outputId": "f22d1e3a-f8ce-4f70-b60c-0782738470ed"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "#Setup data path\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\" # images from a subset of classes from the Food101 dataset\n",
        "\n",
        "# If the image folder doesn't exist, donwload it and prepare it\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exists, skipping redownload.\")\n",
        "else:\n",
        "  print(f\"Did not find {image_path}, downloading it...\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  #Download Pizza, steak, sushi data\n",
        "  with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/refs/heads/main/data/pizza_steak_sushi.zip\")\n",
        "    print(\"downloading pizza, steak, sushi data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "  # Unzip pizza, steak, sushi data\n",
        "  with zipfile.ZipFile(data_path/ \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping pizza, steak, sushi data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "  # Remove zip file\n",
        "  os.remove(data_path / \"pizza_steak_sushi.zip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xDYqttlcE8A",
        "outputId": "9d3cdb86-9890-4da3-c1ee-88f7c876f367"
      },
      "outputs": [],
      "source": [
        "# Setup directory path\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHPFP6occLze"
      },
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "\n",
        "Now we've got some data, want to turn it into PyTorch DataLoaders.\n",
        "\n",
        "To do so, we can use `data_setup.py` and `create_dataloaders()` function we made in 05. Time to use them.\n",
        "\n",
        "There's one thing we have to think about about handling data: how to **transform** it?\n",
        "\n",
        "With `torchvision` 0.13+ we can do two ways:\n",
        "1. Manually created transforms. - You define what transforms to use.\n",
        "2. Automatically created transforms. - The transforms for your data are defined by the model used.\n",
        "\n",
        "Important point: When using a pretrained model, it's important that the data (including your custom data) that you pass through it is **transformed** in the same way that the data the model was trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBIGuGHcdte3"
      },
      "source": [
        "### 2.1 Creating a transform for `torchvision.models` (manual creation).\n",
        "\n",
        "`torchvision.models` contains pretrained models (models ready for transfer learning) right within `torchvision`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf7JNs39c6ye"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "normalize = transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
        "                                 std = [0.229, 0.224, 0.225])\n",
        "\n",
        "manual_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # resize image to 224, 225\n",
        "    transforms.ToTensor(), # get images into range [0, 1]\n",
        "    normalize # make sure images have the same distributions as Imagenet (where pretrained models are trained).\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV9f273hfErP",
        "outputId": "97fa5b0c-3582-472b-ce32-5b88e70ad207"
      },
      "outputs": [],
      "source": [
        "from going_modular.going_modular import data_setup\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                test_dir = test_dir,\n",
        "                                                                                transform = manual_transform,\n",
        "                                                                                batch_size = 32)\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLJzhWDYgLy8"
      },
      "source": [
        "### 2.2 Creating a transform for `torchvision.models` (auto creation)\n",
        "\n",
        "As of `torchvision` v0.13+ there is now support for automatic data transform creation based on the pretrained model weights you are using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IYDK2H4Tg5Ph",
        "outputId": "a286518a-3ad0-4810-de4a-e6e59a7ce09a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUnBoMX4g7yU",
        "outputId": "6be3fa0d-4a30-4eae-e8c0-925556a4effd"
      },
      "outputs": [],
      "source": [
        "# Get a set of pretrained model weights\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # 'Defaults' = best available weights\n",
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPKq_hFDbDNu",
        "outputId": "a8bfd44c-7d27-44f6-dfaf-6202ce21193b"
      },
      "outputs": [],
      "source": [
        "# Get the transforms used to create our pretrained weights\n",
        "auto_transforms = weights.transforms()\n",
        "auto_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yenm4t97bT0Z",
        "outputId": "4918e5b3-da91-4d3a-d89d-650aacb664c1"
      },
      "outputs": [],
      "source": [
        "# Create Dataloaders using automatic transforns\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform = auto_transforms,\n",
        "                                                                               batch_size = 32)\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHat25g6bwD-"
      },
      "source": [
        "## 3. Getting a pretrained model\n",
        "\n",
        "There are various places to get a pretrained model, such as:\n",
        "1. PyTorch domain libraries.\n",
        "2. Libraries like `timm` (torch image models).\n",
        "3. Huggingface Hub (for plenty of different models).`\n",
        "4. Paperswithcode (for models across different problem spaces/domains)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFFwt2zbcmG5"
      },
      "source": [
        "### 3.1  Which pretrained model should you usr?\n",
        "\n",
        "*Experiment, experimient, experiment!*\n",
        "\n",
        "The whole idea of transfer learning: take an already well-performing model from a problem space similar to your own and then customize to your own problem.\n",
        "\n",
        "Three things to consider:\n",
        "1. Speed - how fast does it need to run?\n",
        "2. Size - how big is the model?\n",
        "3. Performance - how well does it go on your chose problem (e.g. how well does it classify images for FoodVision mini).\n",
        "\n",
        "Where does the model live?\n",
        "\n",
        "Is it on device? (like a self-driving car).\n",
        "\n",
        "Looking at: https://docs.pytorch.org/vision/main/models.html#table-of-all-available-classification-weights\n",
        "\n",
        "which model should we chose?\n",
        "\n",
        "For our case (deplying FoodVision Mini on a mobile device), it looks like EffNetB0 is one of our best options in terms performance vs size.\n",
        "\n",
        "However, in light of The Bitter Lesson, if we had infinite compute, we'd likely pick the biggest model + most parameters + most general we could."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLRV8UCUc0jD"
      },
      "source": [
        "### 3.2 Setting up a pretrained model\n",
        "\n",
        "Let's create an instance of EfficientNetB0.\n",
        "\n",
        "Link: https://docs.pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b0.html#torchvision.models.EfficientNet_B0_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuSFhQbNfjWC",
        "outputId": "b95c8af8-0afe-403f-c9eb-ce9d3438f9ff"
      },
      "outputs": [],
      "source": [
        "# OLD method of creating a pretrained model (prior to torchvision v0.13)\n",
        "# model = torchvision.models.efficientnet_b0(pretrained = True).to(device)\n",
        "\n",
        "# New method of creating a pretrained model (torchvision v0.13+)\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "model = torchvision.models.efficientnet_b0(weights = weights).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa8W3PP3f66Q",
        "outputId": "a79c446f-6b5c-429a-95a8-84e35306c720"
      },
      "outputs": [],
      "source": [
        "model.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdm_yb8ShLgG",
        "outputId": "4b6b1fb9-cf21-4f76-c7f8-380d4e4c0e7f"
      },
      "outputs": [],
      "source": [
        "model.avgpool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcRtu7W5hNeC",
        "outputId": "058b16c9-a7a4-492f-9709-55ede9138293"
      },
      "outputs": [],
      "source": [
        "model.classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hucyjE7hP7e"
      },
      "source": [
        "### 3.3 Getting a summary of our model with `torchinfo.summary()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB0sf2i0yKUv",
        "outputId": "0931c997-cb9b-44e7-a462-432716c9701d"
      },
      "outputs": [],
      "source": [
        "# Print a summary with torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "summary(model = model,\n",
        "        input_size=(1, 3, 224, 224), # Example of [batch_size, color_channels, height, width]))\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0bMda-QyqgN"
      },
      "source": [
        "### 3.4 Freezing the base model and changing the output layer to suit our needs\n",
        "\n",
        "With a feature extractor model, typically you will \"freeze\" the base layers of a pretrained/foundational model and update the output layers to suit yout own problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrFuvyut03Ll"
      },
      "outputs": [],
      "source": [
        "# Freeze all of the base layers in EffNetB0\n",
        "for param in model.features.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CcA9eXB1zNa",
        "outputId": "54856bf5-dc9a-4161-ae13-d0d6c9328c78"
      },
      "outputs": [],
      "source": [
        "# Update the classifier head of our model to suit our problem\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.2, inplace=True),\n",
        "    nn.Linear(in_features=1280, # Feature vector coming in\n",
        "              out_features=len(class_names), # How many classes do we have? 3!\n",
        "              bias=True)\n",
        ").to(device)\n",
        "\n",
        "model.classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REo562pu1JcC",
        "outputId": "5246dc73-dbd4-44cd-f47f-04bdb23e88ba"
      },
      "outputs": [],
      "source": [
        "summary(model = model,\n",
        "        input_size=(1, 3, 224, 224), # Example of [batch_size, color_channels, height, width]))\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfAzQba13_MA"
      },
      "source": [
        "## 4. Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VK5kgfE4BBn"
      },
      "outputs": [],
      "source": [
        "# Define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "04462918de9a4178837ab2fd4a68281e",
            "3c2e9591fec24651b28499ab75e9201e",
            "1efafc5e0ca44bbba16a5c94d7d6d669",
            "2ebe1ba134694cadad04ada24aa0cbd5",
            "eb6049d0f63c4d7fbb1d273a10c5f0fc",
            "dab15b5f467844c5a55df15b03c10157",
            "783309d30f234a8da4a7d3768ef92622",
            "9bc36b34ec4248b8a3dd06f02a763d91",
            "c4ac3a9ea20747ce90b5f438f1e792cb",
            "850cf4926ca8426db99d4c070ec98805",
            "b50a1c60601e4dfbbb3c47bb6b559814"
          ]
        },
        "id": "XUUORipr4QBR",
        "outputId": "a0749015-c8ea-4b56-ace9-3a8fdc36f5bc"
      },
      "outputs": [],
      "source": [
        "# Import train function\n",
        "from going_modular.going_modular import engine\n",
        "\n",
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "results = engine.train(model = model,\n",
        "                       train_dataloader = train_dataloader,\n",
        "                       test_dataloader = test_dataloader,\n",
        "                       optimizer = optimizer,\n",
        "                       loss_fn = loss_fn,\n",
        "                       epochs = 5,\n",
        "                       device = device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRis6dLl4vt-"
      },
      "source": [
        "## 5. Evaluate model by ploting loss curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76ZMdfXR586G",
        "outputId": "09597206-16f4-4f18-c787-4bcfbc68efb9"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from helper_functions import plot_loss_curves\n",
        "  print(\"failed\")\n",
        "except:\n",
        "  import requests\n",
        "  print(\"[INFO] Couldn't find helper_functions.py, downloading it...\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/refs/heads/main/helper_functions.py\")\n",
        "    f.write(request.content)\n",
        "\n",
        "  from helper_functions import plot_loss_curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvxYG-Uy7A8w"
      },
      "source": [
        "Let's see if we have an ideal loss curve:\n",
        "https://www.learnpytorch.io/04_pytorch_custom_datasets/#8-what-should-an-ideal-loss-curve-look-like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "KyBpqpnb6EUl",
        "outputId": "145031af-3a72-4baa-b09d-3677daf4559b"
      },
      "outputs": [],
      "source": [
        "# Plot the loss curves of our model\n",
        "plot_loss_curves(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjZkkWGq6vCc"
      },
      "source": [
        "## 6. Make predictions on images from the test set\n",
        "\n",
        "Let's adhere to the data explorer's motto of *visualize, visualize, visualize*!\n",
        "\n",
        "And make some qualitative predictions on our test set.\n",
        "\n",
        "Some things to keep in mind when making predictions/inference on test data/custom data.\n",
        "\n",
        "We have to make sure thaty our test/custom data is:\n",
        "* Same shape - images need to be same shape as model was trained on.\n",
        "* Same datatype -  custom data should be in the same data type.\n",
        "* Same device - custom data/test data should be on the same device as the model.\n",
        "* Same transform -  if you've transformed your custom data, ideally you will transform the test data and custom data the same.\n",
        "\n",
        "To do all of this automatically, let's create a function called `pred_and_plot_image()`:\n",
        "\n",
        "The function will be similar to the one here: https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function\n",
        "\n",
        "1. Take in a trained model, a list of class names, a filepath to a target image, an image size, a transform, and a target device.\n",
        "2. Open the image with `PIL.Image.Open()`\n",
        "3. Create a transform if one doesn't exist.\n",
        "4. Make sure the model is on the target device.\n",
        "5. Turn the model to `model.eval()` model to make sure it's ready for inference (this will turn off things like `nn.Dropout()` )\n",
        "6. Transform the target image and make sure its dimensionality is suited for the mdoel (this mainly relates to batch size)\n",
        "7. Make a prediction on the image by passing to the model.\n",
        "8. Convert the model's output logits to prediction probabilities using `torch.softmax()`\n",
        "9. Convert model's prediction probabilities to prediction labels using `torch.argmax()`\n",
        "10. Plot the image with `matplotlib` and set the ttitle to the prediction label from step 9 and prediction probability from step 8.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfuDu58E76CD"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "#1. Take in a rained model...\n",
        "def pred_and_plot_image(model:torch.nn.Module,\n",
        "                        image_path:str,\n",
        "                        class_names: List[str],\n",
        "                        image_size: Tuple[int, int] = (224, 224),\n",
        "                        transform:torchvision.transforms = None,\n",
        "                        device: torch.device = device):\n",
        "  # 2. Open the Image with PIL\n",
        "  img = Image.open(image_path)\n",
        "\n",
        "  # 3. Create a transform if one doesn't exist\n",
        "  if transform is not None:\n",
        "    image_transform = transform\n",
        "  else:\n",
        "    image_transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
        "                                 std = [0.229, 0.224, 0.225])\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  ### Predict on image ###\n",
        "  # 4. Make sure the model is inside the device\n",
        "  model.to(device)\n",
        "\n",
        "  # 5. Turn on inference mode and eval mode\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    # 6. Transform the image and add an extra batch dimension\n",
        "    transformed_image = image_transform(img).unsqueeze(dim = 0)\n",
        "\n",
        "    # 7. Make a predction on the transformed image by passing it to the model (also ensure it's on the target device)\n",
        "    target_image_pred = model(transformed_image.to(device))\n",
        "\n",
        "  # 8. Convert model's logits to pred probs\n",
        "  target_image_pred_probs = torch.softmax(target_image_pred, dim = 1)\n",
        "\n",
        "  # 9. Convert pred probs to pred labels\n",
        "  target_image_pred_label = torch.argmax(target_image_pred_probs, dim = 1)\n",
        "\n",
        "  # 10. Plot the image and set title to the prediction label\n",
        "  plt.figure()\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n",
        "  plt.axis(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grKCcvFHnV94",
        "outputId": "70b90fb6-56d5-4f47-b480-b50ae3d33b61"
      },
      "outputs": [],
      "source": [
        "# Get a random list subset of image paths from the test set\n",
        "import random\n",
        "num_images_to_plot = 3\n",
        "test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "len(test_image_path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi0up3OJnvKh",
        "outputId": "e5c511a0-a244-413f-9faa-8a442fcc6056"
      },
      "outputs": [],
      "source": [
        "test_image_path_sample = random.sample(population = test_image_path_list,\n",
        "                                      k=num_images_to_plot)\n",
        "test_image_path_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TKoJbLK2n52-",
        "outputId": "96a91c1f-6096-4c9c-ce87-23c1c211665b"
      },
      "outputs": [],
      "source": [
        "# Make prediction on the plot images\n",
        "for image_path in test_image_path_sample:\n",
        "  pred_and_plot_image(model = model,\n",
        "                      image_path = image_path,\n",
        "                      class_names = class_names,\n",
        "                      image_size = (224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKlfyTQAoNqX"
      },
      "source": [
        "### 6.1 Making predictions on a custom image\n",
        "\n",
        "Let's make a prediction on the pizza dad image - https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/04-pizza-dad.jpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRUNcAdqSrP0",
        "outputId": "7163f0db-bf9c-4a7d-a99f-5443f180a4bf"
      },
      "outputs": [],
      "source": [
        "# Donwload the image\n",
        "import requests\n",
        "\n",
        "#setup the custom image path\n",
        "custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n",
        "\n",
        "#Donwload the image if it doesnt exist\n",
        "if not custom_image_path.is_file():\n",
        "  with open(custom_image_path, \"wb\") as f:\n",
        "    #Donwload the image from Github with \"raw\" link\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/04-pizza-dad.jpeg?raw=true\")\n",
        "    print(f\"Download {custom_image_path}...\")\n",
        "    f.write(request.content)\n",
        "else:\n",
        "  print(f\"{custom_image_path} already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "iki2LsbpT_nl",
        "outputId": "398163f2-fd8c-4d01-8b09-17755118896f"
      },
      "outputs": [],
      "source": [
        "# Predict on custom image\n",
        "pred_and_plot_image(model=model,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names = class_names)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOjYemNxi4lMtRIugCKthEa",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04462918de9a4178837ab2fd4a68281e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c2e9591fec24651b28499ab75e9201e",
              "IPY_MODEL_1efafc5e0ca44bbba16a5c94d7d6d669",
              "IPY_MODEL_2ebe1ba134694cadad04ada24aa0cbd5"
            ],
            "layout": "IPY_MODEL_eb6049d0f63c4d7fbb1d273a10c5f0fc"
          }
        },
        "1efafc5e0ca44bbba16a5c94d7d6d669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bc36b34ec4248b8a3dd06f02a763d91",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4ac3a9ea20747ce90b5f438f1e792cb",
            "value": 5
          }
        },
        "2ebe1ba134694cadad04ada24aa0cbd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_850cf4926ca8426db99d4c070ec98805",
            "placeholder": "​",
            "style": "IPY_MODEL_b50a1c60601e4dfbbb3c47bb6b559814",
            "value": " 5/5 [00:14&lt;00:00,  2.58s/it]"
          }
        },
        "3c2e9591fec24651b28499ab75e9201e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dab15b5f467844c5a55df15b03c10157",
            "placeholder": "​",
            "style": "IPY_MODEL_783309d30f234a8da4a7d3768ef92622",
            "value": "100%"
          }
        },
        "783309d30f234a8da4a7d3768ef92622": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "850cf4926ca8426db99d4c070ec98805": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bc36b34ec4248b8a3dd06f02a763d91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b50a1c60601e4dfbbb3c47bb6b559814": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4ac3a9ea20747ce90b5f438f1e792cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dab15b5f467844c5a55df15b03c10157": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb6049d0f63c4d7fbb1d273a10c5f0fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
